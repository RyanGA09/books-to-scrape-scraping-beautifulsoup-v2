{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BfS4\n",
    "import wget\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk sanitasi nama folder atau file (menghilangkan karakter yang tidak valid)\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menyimpan gambar dalam folder kategori\n",
    "def save_image(title, image_url, category):\n",
    "    # Tentukan path folder berdasarkan kategori\n",
    "    path = f\"images/category/{category}/\"\n",
    "    \n",
    "    # Membuat folder kategori jika belum ada\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Debugging kategori\n",
    "        print(f\"Category: {category}\")  # Memastikan kategori yang diambil benar\n",
    "        \n",
    "        # Sanitasi nama file untuk menghindari karakter tidak valid\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        image_filename = f\"{path}{sanitized_title}.jpg\"  # Menentukan nama file gambar yang disimpan\n",
    "        \n",
    "        # Cek apakah gambar sudah ada di folder kategori\n",
    "        if not os.path.exists(image_filename):\n",
    "            # Mengunduh gambar menggunakan wget\n",
    "            wget.download(image_url, image_filename, bar=None)\n",
    "            print(f\"Image for {sanitized_title} saved in {category} folder.\")\n",
    "        else:\n",
    "            print(f\"Image for {sanitized_title} already exists, skipping download.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image for {title}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all links of the categories even for multiple pages:\n",
    "def scraping_category():\n",
    "    print(\"----------start category----------\")\n",
    "    print(\" Please wait ... \")\n",
    "    url = \"http://books.toscrape.com/\"\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        # create a list for all links of the categories:\n",
    "        links_of_categories_all = []\n",
    "        soup = BfS4(response.content, \"html.parser\")\n",
    "        # take information for the sidebar: categories\n",
    "        categories = soup.select(\".side_categories a\")\n",
    "        for category in categories:\n",
    "            href = category[\"href\"]\n",
    "            link = f\"http://books.toscrape.com/{href}\"\n",
    "            # create one link of each book:\n",
    "            links_of_categories_all.append(link)\n",
    "\n",
    "            # start from the second link, start with Travel:\n",
    "            if not href == \"catalogue/category/books_1/index.html\":\n",
    "                response = requests.get(link)\n",
    "                if response.ok:\n",
    "                    soup = BfS4(response.content, \"html.parser\")\n",
    "                    # check if for a next page, take the info: page 1 of 2:\n",
    "                    next_page = soup.findAll('ul', class_='pager')\n",
    "                    if next_page:\n",
    "                        for page in next_page:\n",
    "                            all_num_page = page.find(\"li\", class_=\"current\").text\n",
    "                            # get the last number of info, to know how many pages will be there:\n",
    "                            num_page = int(all_num_page.strip()[10:])\n",
    "\n",
    "                            counter = 2\n",
    "                            while num_page > 1:\n",
    "                                link_next_page = f\"{link.replace('index.html', '')}page-{counter}.html\"\n",
    "                                links_of_categories_all.append(link_next_page)\n",
    "                                num_page -= 1\n",
    "                                counter += 1\n",
    "\n",
    "        # start from the second link in the list:\n",
    "        links_of_categories = links_of_categories_all[1:]\n",
    "        # all links including multiple pages\n",
    "        return links_of_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all links of the books in one category:\n",
    "def scrape_links_of_books_in_category(category_links):\n",
    "    print(\"----------start books in category----------\")\n",
    "    print(\" Please wait ... \")\n",
    "    # read information to get the book link of each book in one category:\n",
    "    # create a list for all links of books inside a category:\n",
    "    books_in_category = []\n",
    "    for link in category_links:\n",
    "        book_url = link.strip()\n",
    "        response = requests.get(book_url)\n",
    "        if response.ok:\n",
    "            soup = BfS4(response.content, \"html.parser\")\n",
    "            # find all <article class=\"product_pod\">:\n",
    "            articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "            for article in articles:\n",
    "                a = article.find(\"a\")\n",
    "                a_link = a[\"href\"]\n",
    "                # create link of each book:\n",
    "                books_in_category.append(\n",
    "                    f'http://books.toscrape.com/catalogue/{a_link.replace(\"../../../\", \"\")}'\n",
    "                )\n",
    "\n",
    "    return books_in_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk scraping data dari setiap halaman buku\n",
    "def scrape_books_from_category_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Memeriksa apakah permintaan berhasil\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BfS4(response.text, 'html.parser')  # Menggunakan built-in html.parser\n",
    "\n",
    "    # Mengambil semua buku\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    book_data = []\n",
    "\n",
    "    for book in books:\n",
    "        # Mengambil judul buku\n",
    "        title = book.h3.a['title']\n",
    "\n",
    "        # Mengambil URL gambar sampul\n",
    "        image = book.find('img')['src']\n",
    "        image_url = image.replace(\"../../\", \"http://books.toscrape.com/\")\n",
    "\n",
    "        # Mengambil URL halaman detail buku\n",
    "        book_link = 'https://books.toscrape.com/catalogue/' + book.h3.a['href'].replace('../../../', '')\n",
    "\n",
    "        # Mengambil kategori buku (menggunakan BeautifulSoup)\n",
    "        category = soup.find(\"a\", attrs={\"href\": re.compile(\"/category/books/\")}).string.strip()\n",
    "        \n",
    "        # Menyimpan data buku dalam bentuk dictionary\n",
    "        book_data.append({\n",
    "            'Title': title,\n",
    "            'Category': category,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "\n",
    "        # Menyimpan gambar menggunakan fungsi dari ImageScraper.py\n",
    "        save_image(title, image_url, category)\n",
    "\n",
    "    return book_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk scraping beberapa halaman\n",
    "def scrape_multiple_pages(base_url, total_pages):\n",
    "    all_books = []\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        if page == 1:\n",
    "            url = base_url  # Halaman pertama\n",
    "        else:\n",
    "            url = f\"{base_url}catalogue/page-{page}.html\"  # Halaman berikutnya\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        books = scrape_books_from_category_page(url)\n",
    "        if books:\n",
    "            all_books.extend(books)\n",
    "        time.sleep(1)  # Memberikan jeda untuk menghindari terlalu banyak request\n",
    "\n",
    "    return all_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb10dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_info(links):\n",
    "    information = []\n",
    "    for link in links:\n",
    "        book_info = scrape_books_from_category_page(link)\n",
    "        information.append(book_info)\n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mengambil data dari halaman detail buku\n",
    "def scrape_book_details(book_link):\n",
    "    try:\n",
    "        response = requests.get(book_link)\n",
    "        response.raise_for_status()  # Memeriksa apakah permintaan berhasil\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {'description': 'No description available', 'price_incl_tax': 'N/A', 'price_excl_tax': 'N/A', 'price_tax': 'N/A'}\n",
    "\n",
    "    soup = BfS4(response.text, 'html.parser')\n",
    "\n",
    "    # Mengambil deskripsi produk\n",
    "    description = soup.find('meta', {'name': 'description'})\n",
    "    description = description['content'] if description else 'No description available'\n",
    "\n",
    "    # Mengambil harga produk (price including tax, price excluding tax, price tax)\n",
    "    price_incl_tax = 'N/A'\n",
    "    price_excl_tax = 'N/A'\n",
    "    price_tax = 'N/A'\n",
    "\n",
    "    price_incl_tax_elem = soup.find('th', text='Price (incl. tax)')\n",
    "    if price_incl_tax_elem:\n",
    "        price_incl_tax = price_incl_tax_elem.find_next_sibling('td').text.strip()\n",
    "\n",
    "    price_excl_tax_elem = soup.find('th', text='Price (excl. tax)')\n",
    "    if price_excl_tax_elem:\n",
    "        price_excl_tax = price_excl_tax_elem.find_next_sibling('td').text.strip()\n",
    "\n",
    "    price_tax_elem = soup.find('th', text='Tax')\n",
    "    if price_tax_elem:\n",
    "        price_tax = price_tax_elem.find_next_sibling('td').text.strip()\n",
    "\n",
    "    return {\n",
    "        'description': description,\n",
    "        'price_incl_tax': price_incl_tax,\n",
    "        'price_excl_tax': price_excl_tax,\n",
    "        'price_tax': price_tax\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mengambil link buku dari katalog\n",
    "def scrape_links_of_books_from_page(page_url):\n",
    "    books_in_page = []\n",
    "    response = requests.get(page_url)\n",
    "    if response.ok:\n",
    "        soup = BfS4(response.content, \"html.parser\")\n",
    "        # Ambil semua artikel dengan kelas \"product_pod\" yang berisi informasi buku\n",
    "        articles = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "        for article in articles:\n",
    "            a = article.find(\"a\")\n",
    "            a_link = a[\"href\"]\n",
    "            # Membuat link lengkap ke halaman detail buku\n",
    "            books_in_page.append(f'http://books.toscrape.com/catalogue/{a_link.replace(\"../../../\", \"\")}')\n",
    "    return books_in_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mengambil data detail satu buku\n",
    "def scrape_book_data(book_link):\n",
    "    print(f\"Scraping {book_link} ...\")\n",
    "    response = requests.get(book_link)\n",
    "    if response.ok:\n",
    "        soup = BfS4(response.content, \"html.parser\")\n",
    "        image = soup.find(\"img\")\n",
    "        image_url = image[\"src\"].replace(\"../../\", \"http://books.toscrape.com/\")  # Mengubah url relatif menjadi absolut\n",
    "        title = image[\"alt\"]\n",
    "        price = soup.find('p', class_='price_color').text\n",
    "        availability = soup.find(\"th\", text=\"Availability\").find_next_sibling(\"td\").string.strip()\n",
    "        rating = soup.find(\"p\", attrs={'class': 'star-rating'}).get(\"class\")[1]\n",
    "        details = scrape_book_details(book_link)\n",
    "        \n",
    "        data = {\n",
    "            \"Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Price including tax\": details['price_incl_tax'],\n",
    "            \"Price excluding tax\": details['price_excl_tax'],\n",
    "            \"Price Tax\": details['price_tax'],\n",
    "            \"Availability\": availability,\n",
    "            \"Product Description\": details['description'],\n",
    "            \"Rating\": rating,\n",
    "            \"Image URL\": image_url,\n",
    "            \"Link\": book_link\n",
    "        }\n",
    "        return data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb48724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk scraping buku dari beberapa halaman katalog\n",
    "def scrape_books_from_pages(base_url, total_pages):\n",
    "    all_books = []\n",
    "    for page in range(1, total_pages + 1):\n",
    "        if page == 1:\n",
    "            url = base_url  # Halaman pertama\n",
    "        else:\n",
    "            url = f\"{base_url}catalogue/page-{page}.html\"  # Halaman berikutnya\n",
    "\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        \n",
    "        # Ambil semua link buku dari halaman ini\n",
    "        books_in_page = scrape_links_of_books_from_page(url)\n",
    "        for book_link in books_in_page:\n",
    "            book_data = scrape_book_data(book_link)\n",
    "            if book_data:\n",
    "                all_books.append(book_data)\n",
    "\n",
    "        time.sleep(1)  # Memberikan jeda untuk menghindari terlalu banyak request\n",
    "\n",
    "    return all_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6181d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menyimpan hasil scraping ke file CSV\n",
    "def save_to_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    # start the program:\n",
    "    # get first all categories with category_scrape:\n",
    "    all_categories = scraping_category()\n",
    "    \n",
    "    # Menyimpan gambar menggunakan fungsi dari ImageScraper.py\n",
    "    # save_image(image_url, title, category)\n",
    "    \n",
    "    links = scrape_links_of_books_in_category(all_categories)\n",
    "    \n",
    "    category_info(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_url = 'http://books.toscrape.com/'  # URL dasar untuk katalog buku\n",
    "    total_pages = 3  # Jumlah halaman yang ingin di-scrape, bisa Anda ubah sesuai kebutuhan\n",
    "    \n",
    "    # Scrape buku dari beberapa halaman\n",
    "    books_data = scrape_books_from_pages(base_url, total_pages)\n",
    "\n",
    "    # Simpan hasil ke file CSV\n",
    "    save_to_csv(books_data, 'books_data.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi utama untuk memulai proses scraping dan menyimpan ke file CSV\n",
    "def main():\n",
    "    base_url = 'http://books.toscrape.com/'  # URL dasar untuk katalog buku\n",
    "    total_pages = 3  # Jumlah halaman yang ingin di-scrape, bisa Anda ubah sesuai kebutuhan\n",
    "\n",
    "    # Scrape buku dari beberapa halaman\n",
    "    all_categories = scraping_category()\n",
    "    links = scrape_links_of_books_in_category(all_categories)\n",
    "    books_data = []\n",
    "    for link in links:\n",
    "        book_data = scrape_book_data(link)\n",
    "        if book_data:\n",
    "            books_data.append(book_data)\n",
    "\n",
    "    # Simpan hasil ke file CSV\n",
    "    save_to_csv(books_data, 'books_data.csv')\n",
    "    print(\"Scraping selesai.\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
